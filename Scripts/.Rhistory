Headers <- cbind(TimeFrame, Stations)
Headers <- cbind("Date", Stations)
Headers <- cbind("Date", Stations)
source("C:/Users/palemi/Documents/Github/DWRAT_DataScraping/Scripts/Downsizer_Processor.R", echo=TRUE)
View(Headers)
Headers = read.csv(file = here("InputData/Downsizer_Stations.csv"))
Headers = Headers %>% add_column("Date")
Downsizer = read.csv(file = here("InputData/2023.1.05.csv"))
Headers = read.csv(file = here("InputData/Downsizer_Stations.csv"))
Headers = Headers %>% add_column("Date")
View(Headers)
View(Headers)
source("C:/Users/palemi/Documents/Github/DWRAT_DataScraping/Scripts/Downsizer_Processor.R", echo=TRUE)
View(Headers)
Headers = read.csv(file = here("InputData/Downsizer_Stations.csv"))
View(Headers)
#Load libraries----
library(dplyr)
library(tidyverse)
library(here)
library(lubridate)
#Extract the Downsizer Headers----
#Import Downsizer_Headers.csv (long names)
Downsizer_Headers = read.csv(here("InputData/Downsizer_Headers.csv"), header = FALSE)
#Remove all forward slashes
Downsizer_Headers = gsub(pattern = "/", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Load libraries----
library(dplyr)
library(tidyverse)
library(here)
library(lubridate)
#Extract the Downsizer Headers----
#Import Downsizer_Headers.csv (long names)
Downsizer_Headers = read.csv(here("InputData/Downsizer_Headers.csv"), header = FALSE)
#Remove all forward slashes
Downsizer_Headers = gsub(pattern = "/", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Remove all substrings "\t"
Downsizer_Headers = gsub(pattern = "\t", replacement =  "", Downsizer_Headers)
Downsizer_Headers
#Remove all blank spaces
Downsizer_Headers = gsub(pattern = " ", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Extract the Downsizer Headers----
#Import Downsizer_Headers.csv (long names)
Downsizer_Headers = read.csv(here("InputData/Downsizer_Headers.csv"), header = FALSE)
#Remove all forward slashes
Downsizer_Headers = gsub(pattern = "/", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Remove all substrings "\t"
Downsizer_Headers = gsub(pattern = "\t", replacement =  "", Downsizer_Headers)
Downsizer_Headers
#Remove all blank spaces
Downsizer_Headers = gsub(pattern = " ", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Rename the column to Stations
colnames(Downsizer_Headers) = "Stations"
#Load libraries----
library(dplyr)
library(tidyverse)
library(here)
library(lubridate)
#Extract the Downsizer Headers----
#Import Downsizer_Headers.csv (long names)
Downsizer_Headers = read.csv(here("InputData/Downsizer_Headers.csv"), header = FALSE)
#Remove all forward slashes
Downsizer_Headers = gsub(pattern = "/", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Remove all substrings "\t"
Downsizer_Headers = gsub(pattern = "\t", replacement =  "", Downsizer_Headers)
Downsizer_Headers
#Remove all blank spaces
Downsizer_Headers = gsub(pattern = " ", replacement = "", Downsizer_Headers)
Downsizer_Headers
#Rename the column to Stations
colnames(Downsizer_Headers) = "Stations"
write.csv(Downsizer_Headers, here("InputData/Downsizer_Stations.csv"), row.names = FALSE)
colnames(Downsizer_Headers) = "Stations"
source("~/Github/DWRAT_DataScraping/Scripts/Downsizer_Headers_Extraction.R", echo=TRUE)
Downsizer_Headers
source("C:/Users/palemi/Documents/Github/DWRAT_DataScraping/Scripts/Downsizer_Processor.R")
## load packages
library(RSelenium)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
library(lubridate)
#InputData----
#Dates--adjust as needed
Stations <- read.csv(here("InputData/CIMIS_Stations.csv"))
StartDate = data.frame("December", "01", "2022", as.Date("2022-12-01"))
EndDate = data.frame("January", "04", "2023", as.Date("2023-01-04"))
colnames(StartDate) = c("month", "day", "year", "date")
colnames(EndDate) = c("month", "day", "year", "date")
ndays = seq(from = StartDate$date, to = EndDate$date, by = 'day')%>% length()
ndays
# Set up RSelenium ----
# Open a chrome browser session with RSelenium
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='108.0.5359.71',
port = free_port(),
)
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
remDr <- rs_driver_object$client
#Create a list to hold CIMIS dataframes
DF_List <- list()
#Navigate to CIMIS----
for (i in 1:nrow(Stations)){
remDr$navigate(paste0("https://ipm.ucanr.edu/calludt.cgi/WXSTATIONDATA?MAP=&STN=", Stations$Alias[i]))
#Input Dates
StartMonth <- remDr$findElement(using = "name", value = "FROMMONTH")
StartMonth$sendKeysToElement(list(StartDate$month))
StartDay <- remDr$findElement(using = "name", value = "FROMDAY")
StartDay$sendKeysToElement(list(StartDate$day))
StartYear <- remDr$findElement(using = "name", value = "FROMYEAR")
StartYear$sendKeysToElement(list(StartDate$year))
EndMonth <- remDr$findElement(using = "name", value = "THRUMONTH")
EndMonth$sendKeysToElement(list(EndDate$month))
EndDay <-remDr$findElement(using = "name", value = "THRUDAY")
EndDay$sendKeysToElement(list(EndDate$day))
EndYear <-remDr$findElement(using = "name", value = "THRUYEAR")
EndYear$sendKeysToElement(list(EndDate$year))
#Use no backups
Backups <- remDr$findElement(using = "name", value = "NONE")
Backups$clickElement()
#Uncheck unnecessary checkboxes
Soil <- remDr$findElement(using = "name", value = "DT_SOIL")
Soil$clickElement()
Wind <- remDr$findElement(using = "name", value = "DT_WIND")
Wind$clickElement()
RH <- remDr$findElement(using = "name", value = "DT_RH")
RH$clickElement()
ET <- remDr$findElement(using = "name", value = "DT_ET")
ET$clickElement()
Solar <- remDr$findElement(using = "name", value = "DT_SOLAR")
Solar$clickElement()
#Metric Units
Metric <- remDr$findElement(using = "xpath", "//input[@value = 'M']")
Metric$clickElement()
#Comma delimited format
Comma <- remDr$findElement(using = "xpath", "//input[@value = 'T']")
Comma$clickElement()
#Retrieve Report
Report <-remDr$findElement(using = "xpath", "//input[@value = 'RETRIEVE DATA']")
Report$clickElement()
#Grab the Data
WeatherData <- remDr$findElement(using = "xpath", "//pre")
WeatherDataText <-WeatherData$getElementText() %>% unlist() %>% data.frame()
#Manipulate CIMIS Data After Download----
WeatherDataBody <- substring(WeatherDataText, 2551, nchar(WeatherDataText))
WeatherDataBody <-gsub("\\\n", " ", WeatherDataBody) #Remove \n from
WeatherDataBody <-gsub(" ", "", WeatherDataBody) #remove blank spaces
WeatherDataBody <- strsplit( WeatherDataBody, ",") %>% unlist %>% data.frame() #split by commas
#Force WeatherDataBody into a dataframe with 19 columns
WeatherDataBody <- split(WeatherDataBody,rep(1:ndays,each=19)) %>% data.frame %>% t() %>% data.frame()
#Drop the last 12 columns
WeatherDataBody <-select(WeatherDataBody, -c(X8:X19))
#Add column headers to WeatherDataBody
Headers <- c("Station","Date","Time","Precip","type","Tmax","Tmin")
colnames(WeatherDataBody) = Headers
#Drop Time and type columns
WeatherDataBody <- select(WeatherDataBody, -c("Time", "type"))
DF_List[[i]] <- WeatherDataBody
}
#Finalize CIMIS Data For Exportation To CSV----
#Name the individual RAWS dataframes in DF_List
names(DF_List) <- lapply(seq_along(DF_List),
function(i) names(DF_List)[[i]] = paste0("CIMIS_", Stations$Station[i]))
#Extract dataframes from DF_List
lapply(names(DF_List), function(i)
assign(x = i, value = DF_List[[i]], .GlobalEnv))
#Finalize CIMIS Sanel Valley 106
CIMIS_Sanel_Valley_106 = `CIMIS_Sanel Valley 106`
rm(`CIMIS_Sanel Valley 106`)
CIMIS_Sanel_Valley_106$Precip = NULL
#Finalize CIMIS Santa Rosa 83
CIMIS_Santa_Rosa_83 = `CIMIS_Santa Rosa 83`
rm(`CIMIS_Santa Rosa 83`)
CIMIS_Santa_Rosa_83$Precip = NULL
#Finalize CIMIS Windsor 103
CIMIS_Windsor_103 = `CIMIS_Windsor 103`
rm(`CIMIS_Windsor 103`)
CIMIS_Windsor_103$Tmin = NULL
CIMIS_Windsor_103$Tmax = NULL
#Finalize CIMIS Hopland 85 (just consists of -999)
CIMIS_Hopland_85 <- cbind.data.frame(seq(from = StartDate$date, to = EndDate$date, by = 'day'), rep(-999,ndays))
colnames(CIMIS_Hopland_85) <- c("Date", "Precipitation")
##Export Dataframes to CSVs----
write.csv(CIMIS_Windsor_103, here("ProcessedData/CIMIS_PRECIP12.csv"), row.names = FALSE)
write.csv(CIMIS_Sanel_Valley_106, here("ProcessedData/CIMIS_TEMP3.csv"), row.names = FALSE)
write.csv(CIMIS_Santa_Rosa_83, here("ProcessedData/CIMIS_TEMP4.csv"), row.names = FALSE)
write.csv(CIMIS_Hopland_85, here("ProcessedData/CIMIS_PRECIP6.csv"), row.names = FALSE)
# Set up RSelenium -------------------------------------------------------
## load packages
library(RSelenium)
library(rvest)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
##Set Default download folder----
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
default_folder = here("WebData")
##Open a chrome browser session with RSelenium----
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='108.0.5359.71',
port = free_port(),
)
remDr <- rs_driver_object$client
#Input Data for both datasets----
#Define Dates
StartDate = data.frame("December", "01", "2022", as.Date("2022-12-01"))
EndDate = data.frame("January", "04", "2023", as.Date("2023-01-04"))
colnames(StartDate) = c("month", "day", "year", "date")
colnames(EndDate) = c("month", "day", "year", "date")
ndays = seq(from = StartDate$date, to = EndDate$date, by = 'day')%>% length()
ndays
#Precipitation Data----
##Input Locations File
PrecipitationFile <- gsub(pattern = "/", replacement = "\\\\", x = here("InputData/prism_rr_precip_stations.csv"))
##Navigate to PRISM Explorer webpage----
remDr <- rs_driver_object$client
URL <- "https://prism.oregonstate.edu/explorer/bulk.php"
remDr$navigate(URL)
##Fill out PRISM Explorer webpage----
#Select precipitation checkbox
# Precipitation <-remDr$findElement(using = "id", value = "cvar_ppt")
# Precipitation$clickElement()
#Deselect all other data settings checkboxes
MeanTemp <- remDr$findElement(using = "id", value = "cvar_tmean")
MeanTemp$clickElement()
#Select daily values
Daily <- remDr$findElement(using = "id", value = "tper_daily")
Daily$clickElement()
#Set the Starting Date
StartDay <- remDr$findElement(using = "id", value = "tper_daily_start_day")
StartDay$sendKeysToElement(list(StartDate$day))
StartMonth <- remDr$findElement(using = "id", value = "tper_daily_start_month")
StartMonth$sendKeysToElement(list(StartDate$month))
StartYear <- remDr$findElement(using = "id", value = "tper_daily_start_year")
StartYear$sendKeysToElement(list(StartDate$year))
#Set the Ending Date
EndDay <- remDr$findElement(using = "id", value = "tper_daily_end_day")
EndDay$sendKeysToElement(list(EndDate$day))
EndMonth <- remDr$findElement(using = "id", value = "tper_monthly_end_month")
EndMonth$sendKeysToElement(list(EndDate$month))
EndYear <- remDr$findElement(using = "id", value = "tper_daily_end_year")
EndYear$sendKeysToElement(list(EndDate$year))
#Select SI (metric) units
Units <- remDr$findElement(using = "id", value = "units_si")
Units$clickElement()
#Check Interpolate grid cell values
Grid <- remDr$findElement(using = "id", value = "loc_interp")
Grid$clickElement()
#Select Open Locations File
UploadBtn <- remDr$findElement(using = "id", value = "upload_locations")
#UploadBtn$sendKeysToElement(list(PrecipitationFile))
UploadBtn$sendKeysToElement(list("C:\\Users\\palemi\\Water Boards\\Supply and Demand Assessment - Documents\\DWRAT_DataScraping\\InputData\\prism_rr_precip_stations.csv"))
UploadBtn$clickElement()
#Click Prepare & Download Time Series
SubmitBtn <- remDr$findElement(using = "id", value = "submitdown_button")
SubmitBtn$clickElement()
#Temperature Data ----
#Import Temperature Stations
TempFile <- gsub(pattern = "/", replacement = "\\\\", x = here("InputData/temp_fill_stations.csv"))
##Navigate to PRISM Explorer webpage----
remDr <- rs_driver_object$client
URL <- "https://prism.oregonstate.edu/explorer/bulk.php"
remDr$navigate(URL)
#Deselect precipitation checkbox
Precipitation <-remDr$findElement(using = "id", value = "cvar_ppt")
Precipitation$clickElement()
#Select Temperature checkboxes
MinTemp <-remDr$findElement(using = "id", value = "cvar_tmin")
MinTemp$clickElement()
MaxTemp <- remDr$findElement(using = "id", value = "cvar_tmax")
MaxTemp$clickElement()
#Select daily values
Daily <- remDr$findElement(using = "id", value = "tper_daily")
Daily$clickElement()
#Set the Starting Date
StartDay <- remDr$findElement(using = "id", value = "tper_daily_start_day")
StartDay$sendKeysToElement(list(StartDate$day))
StartMonth <- remDr$findElement(using = "id", value = "tper_daily_start_month")
StartMonth$sendKeysToElement(list(StartDate$month))
StartYear <- remDr$findElement(using = "id", value = "tper_daily_start_year")
StartYear$sendKeysToElement(list(StartDate$year))
#Set the Ending Date
EndDay <- remDr$findElement(using = "id", value = "tper_daily_end_day")
EndDay$sendKeysToElement(list(EndDate$day))
EndMonth <- remDr$findElement(using = "id", value = "tper_monthly_end_month")
EndMonth$sendKeysToElement(list(EndDate$month))
EndYear <- remDr$findElement(using = "id", value = "tper_daily_end_year")
EndYear$sendKeysToElement(list(EndDate$year))
#Select SI (metric) units
Units <- remDr$findElement(using = "id", value = "units_si")
Units$clickElement()
#Check Interpolate grid cell values
Grid <- remDr$findElement(using = "id", value = "loc_interp")
Grid$clickElement()
#Select Open Locations File
UploadBtn <- remDr$findElement(using = "id", value = "upload_locations")
UploadBtn$sendKeysToElement(list(TempFile))
UploadBtn$clickElement()
#Click Prepare & Download Time Series
SubmitBtn <- remDr$findElement(using = "id", value = "submitdown_button")
SubmitBtn$clickElement()
#Set up RSelenium ----
## load packages ----
library(RSelenium)
library(rvest)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
##Set Default download folder ----
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
default_folder = here("WebData")
## Open browser ----
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='108.0.5359.71',
port = free_port(),
)
remDr <- rs_driver_object$client
#Input Data----
#Import RAWS stations
Stations = read.csv("InputData/Raws_Stations.csv")
## load packages
library(RSelenium)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
library(lubridate)
#InputData----
#Dates--adjust as needed
Stations <- read.csv(here("InputData/CIMIS_Stations.csv"))
StartDate = data.frame("February", "01", "2023", as.Date("2023-02-01"))
EndDate = data.frame("March", "13", "2023", as.Date("2023-03-04"))
colnames(StartDate) = c("month", "day", "year", "date")
colnames(EndDate) = c("month", "day", "year", "date")
ndays = seq(from = StartDate$date, to = EndDate$date, by = 'day')%>% length()
ndays
# Set up RSelenium ----
# Open a chrome browser session with RSelenium
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='108.0.5359.71',
port = free_port(),
)
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
remDr <- rs_driver_object$client
#Create a list to hold CIMIS dataframes
DF_List <- list()
#Navigate to CIMIS----
for (i in 1:nrow(Stations)){
#i=1
remDr$navigate(paste0("https://ipm.ucanr.edu/calludt.cgi/WXSTATIONDATA?MAP=&STN=", Stations$Alias[i]))
#Input Dates
StartMonth <- remDr$findElement(using = "name", value = "FROMMONTH")
StartMonth$sendKeysToElement(list(StartDate$month))
StartDay <- remDr$findElement(using = "name", value = "FROMDAY")
StartDay$sendKeysToElement(list(StartDate$day))
StartYear <- remDr$findElement(using = "name", value = "FROMYEAR")
StartYear$sendKeysToElement(list(StartDate$year))
EndMonth <- remDr$findElement(using = "name", value = "THRUMONTH")
EndMonth$sendKeysToElement(list(EndDate$month))
EndDay <-remDr$findElement(using = "name", value = "THRUDAY")
EndDay$sendKeysToElement(list(EndDate$day))
EndYear <-remDr$findElement(using = "name", value = "THRUYEAR")
EndYear$sendKeysToElement(list(EndDate$year))
#Use no backups
Backups <- remDr$findElement(using = "name", value = "NONE")
Backups$clickElement()
#Uncheck unnecessary checkboxes
Soil <- remDr$findElement(using = "name", value = "DT_SOIL")
Soil$clickElement()
Wind <- remDr$findElement(using = "name", value = "DT_WIND")
Wind$clickElement()
RH <- remDr$findElement(using = "name", value = "DT_RH")
RH$clickElement()
ET <- remDr$findElement(using = "name", value = "DT_ET")
ET$clickElement()
Solar <- remDr$findElement(using = "name", value = "DT_SOLAR")
Solar$clickElement()
#Metric Units
Metric <- remDr$findElement(using = "xpath", "//input[@value = 'M']")
Metric$clickElement()
#Comma delimited format
Comma <- remDr$findElement(using = "xpath", "//input[@value = 'T']")
Comma$clickElement()
#Retrieve Report
Report <-remDr$findElement(using = "xpath", "//input[@value = 'RETRIEVE DATA']")
Report$clickElement()
#Grab the Data
WeatherData <- remDr$findElement(using = "xpath", "//pre")
WeatherDataText <-WeatherData$getElementText() %>% unlist() %>% data.frame()
#Manipulate CIMIS Data After Download----
WeatherDataBody <- substring(WeatherDataText, 2551, nchar(WeatherDataText))
WeatherDataBody <-gsub("\\\n", " ", WeatherDataBody) #Remove \n from
WeatherDataBody <-gsub(" ", "", WeatherDataBody) #remove blank spaces
WeatherDataBody <- strsplit( WeatherDataBody, ",") %>% unlist %>% data.frame() #split by commas
#Force WeatherDataBody into a dataframe with 19 columns
WeatherDataBody <- split(WeatherDataBody,rep(1:(nrow(WeatherDataBody)/19),each=19)) %>% data.frame %>% t() %>% data.frame()
#Drop the last 12 columns
WeatherDataBody <-select(WeatherDataBody, -c(X8:X19))
#Add column headers to WeatherDataBody
Headers <- c("Station","Date","Time","Precip","type","Tmax","Tmin")
colnames(WeatherDataBody) = Headers
#Drop Time and type columns
WeatherDataBody <- select(WeatherDataBody, -c("Time", "type"))
DF_List[[i]] <- WeatherDataBody
}
i=1
remDr$navigate(paste0("https://ipm.ucanr.edu/calludt.cgi/WXSTATIONDATA?MAP=&STN=", Stations$Alias[i]))
remDr$open()
# Set up RSelenium -------------------------------------------------------
## load packages ----
library(RSelenium)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
# Import CNRFC Temperature stations----
CNRFC_Stations <- read.csv("InputData/CNRFC_Stations.csv")
# Set up RSelenium -------------------------------------------------------
## load packages ----
library(RSelenium)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
# Import CNRFC Temperature stations----
CNRFC_Stations <- read.csv("InputData/CNRFC_Stations.csv")
CNRFC_Stations <- read.csv("InputData/CNRFC_Stations.csv")
library(readr)
CNRFC_Stations <- read_csv("~/Github/DWRAT_DataScraping/InputData/CNRFC_Stations.csv")
View(CNRFC_Stations)
here()
here("InputData")
CNRFC_Stations <- read.csv(here("InputData/CNRFC_Stations.csv"))
##Set Default download folder ----
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
default_folder <- eCaps$chromeOptions$prefs$download.default_directory
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='108.0.5359.71',
port = free_port(),
extraCapabilities = eCaps
)
remDr <- rs_driver_object$client
remDr$open()
binman::list_versions('chromedriver')
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='111.0.5563.64', #set to the version on your PC that most closely matches the chrome browser version
port = free_port(),
extraCapabilities = eCaps
)
remDr <- rs_driver_object$client
remDr$open()
#write to CSV
write.csv(Dat_Shell, here("InputData\"Dat_Shell.csv"), row.names = FALSE)
#Install and load libraries
library(dplyr)
library(tidyverse)
library(here)
library(lubridate) #for make_date function
#Import DAT stuff
DAT_File <- read.delim(here("InputData/data_update_to_11.30.2022.txt"), sep = "\t")
DAT_Fields <- read.csv(here("InputData/DAT_Fields.csv"))
#Set DAT_File column names
colnames(DAT_File) <- colnames(DAT_Fields)
#Notes about DAT_File fields----
#22 Runoff fields, always equal 1
#6 date-time fields, pre-filled from 1/1/1990 through 9/30/2023
#15 precipitation fields
#8 temperature fields
#We will replace a subset of the data corresponding to our timeframe of interest
#Whittle the DAT_File to a subset corresponding to our timeframe of interest, "DAT_Shell"----
#Add a Timestep column
DAT_File$TimeStep <- make_date(year = DAT_File$Year,
month = DAT_File$month,
day = DAT_File$day)
#Filter to the timeframe of interest
#Set the start date by grabbing the first day of the current month
#Start_Date <- lubridate::floor_date(Sys.Date(), unit = "month")
Start_Date <- as.Date("2023-01-11")
#The end date is the current date + 5 days in the future; we grab 6 days of forecast data from CNRFC
End_Date <- Sys.Date() + 5
DAT_Shell <- subset(DAT_File, TimeStep >= '2023-01-11' & TimeStep <= "2023-03-22") #Adjust as needed
DAT_Shell
#Set TimeStep as the 7th column in DAT_Shell
DAT_Shell <- DAT_Shell %>% relocate(TimeStep, .after = s)
#Set all the DAT_Shell temperature and precipitation fields to blank
for (i in 1:length(DAT_Shell)){
if (colnames(DAT_Shell[i]) %>% str_detect("PREC")){
DAT_Shell[i] = ""
} else if (colnames(DAT_Shell[i]) %>% str_detect("TM")){
DAT_Shell[i] = ""
}else {
DAT_Shell[i] = DAT_Shell[i]
}
}
write.csv(Dat_Shell, here("InputData\"Dat_Shell.csv"), row.names = FALSE)
write.csv(Dat_Shell, here("InputData\"Dat_Shell.csv"), row.names = FALSE)
write.csv(DAT_Shell, here("InputData\"Dat_Shell.csv"), row.names = FALSE)
write.csv(DAT_Shell, here("InputData\Dat_Shell.csv"), row.names = FALSE)
write.csv(DAT_Shell, here("InputData/Dat_Shell.csv"), row.names = FALSE)
git branch
## load packages
library(RSelenium)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
library(lubridate)
#InputData----
#Dates--adjust as needed; EndDate is always yesterday
Stations <- read.csv(here("InputData/CIMIS_Stations.csv"))
StartDate = data.frame("January", "11", "2023", as.Date("2023-01-11"))
EndDate = data.frame("March", "16", "2023", as.Date("2023-03-16"))
colnames(StartDate) = c("month", "day", "year", "date")
colnames(EndDate) = c("month", "day", "year", "date")
ndays = seq(from = StartDate$date, to = EndDate$date, by = 'day')%>% length()
ndays
# Set up RSelenium ----
# Open a chrome browser session with RSelenium
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='111.0.5563.64',
port = free_port(),
)
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
remDr <- rs_driver_object$client
remDr$open()
#Create a list to hold CIMIS dataframes
DF_List <- list()
#Navigate to CIMIS----
for (i in 1:nrow(Stations)){
#i=1
URL <- paste0("https://ipm.ucanr.edu/calludt.cgi/WXSTATIONDATA?MAP=&STN=", Stations$Alias[i])
URL <- toString(URL)
remDr$navigate(URL)
#Input Dates
StartMonth <- remDr$findElement(using = "name", value = "FROMMONTH")
StartMonth$sendKeysToElement(list(StartDate$month))
StartDay <- remDr$findElement(using = "name", value = "FROMDAY")
StartDay$sendKeysToElement(list(StartDate$day))
StartYear <- remDr$findElement(using = "name", value = "FROMYEAR")
StartYear$sendKeysToElement(list(StartDate$year))
EndMonth <- remDr$findElement(using = "name", value = "THRUMONTH")
EndMonth$sendKeysToElement(list(EndDate$month))
EndDay <-remDr$findElement(using = "name", value = "THRUDAY")
EndDay$sendKeysToElement(list(EndDate$day))
EndYear <-remDr$findElement(using = "name", value = "THRUYEAR")
EndYear$sendKeysToElement(list(EndDate$year))
#Use no backups
Backups <- remDr$findElement(using = "name", value = "NONE")
Backups$clickElement()
#Uncheck unnecessary checkboxes
Soil <- remDr$findElement(using = "name", value = "DT_SOIL")
Soil$clickElement()
Wind <- remDr$findElement(using = "name", value = "DT_WIND")
Wind$clickElement()
RH <- remDr$findElement(using = "name", value = "DT_RH")
RH$clickElement()
ET <- remDr$findElement(using = "name", value = "DT_ET")
ET$clickElement()
Solar <- remDr$findElement(using = "name", value = "DT_SOLAR")
Solar$clickElement()
#Metric Units
Metric <- remDr$findElement(using = "xpath", "//input[@value = 'M']")
Metric$clickElement()
#Comma delimited format
Comma <- remDr$findElement(using = "xpath", "//input[@value = 'T']")
Comma$clickElement()
#Retrieve Report
Report <-remDr$findElement(using = "xpath", "//input[@value = 'RETRIEVE DATA']")
Report$clickElement()
#Grab the Data
WeatherData <- remDr$findElement(using = "xpath", "//pre")
WeatherDataText <-WeatherData$getElementText() %>% unlist() %>% data.frame()
#Manipulate CIMIS Data After Download----
WeatherDataBody <- substring(WeatherDataText, 2551, nchar(WeatherDataText))
WeatherDataBody <-gsub("\\\n", " ", WeatherDataBody) #Remove \n from
WeatherDataBody <-gsub(" ", "", WeatherDataBody) #remove blank spaces
WeatherDataBody <- strsplit( WeatherDataBody, ",") %>% unlist %>% data.frame() #split by commas
#Force WeatherDataBody into a dataframe with 19 columns
WeatherDataBody <- split(WeatherDataBody,rep(1:(nrow(WeatherDataBody)/19),each=19)) %>% data.frame %>% t() %>% data.frame()
#Drop the last 12 columns
WeatherDataBody <-select(WeatherDataBody, -c(X8:X19))
#Add column headers to WeatherDataBody
Headers <- c("Station","Date","Time","Precip","type","Tmax","Tmin")
colnames(WeatherDataBody) = Headers
#Drop Time and type columns
WeatherDataBody <- select(WeatherDataBody, -c("Time", "type"))
DF_List[[i]] <- WeatherDataBody
}
#Finalize CIMIS Data For Exportation To CSV----
#Name the individual RAWS dataframes in DF_List
names(DF_List) <- lapply(seq_along(DF_List),
function(i) names(DF_List)[[i]] = paste0("CIMIS_", Stations$Station[i]))
#Extract dataframes from DF_List
lapply(names(DF_List), function(i)
assign(x = i, value = DF_List[[i]], .GlobalEnv))
#Finalize CIMIS Sanel Valley 106
CIMIS_Sanel_Valley_106 = `CIMIS_Sanel Valley 106`
rm(`CIMIS_Sanel Valley 106`)
CIMIS_Sanel_Valley_106$Precip = NULL
#Finalize CIMIS Santa Rosa 83
CIMIS_Santa_Rosa_83 = `CIMIS_Santa Rosa 83`
rm(`CIMIS_Santa Rosa 83`)
CIMIS_Santa_Rosa_83$Precip = NULL
#Finalize CIMIS Windsor 103
CIMIS_Windsor_103 = `CIMIS_Windsor 103`
rm(`CIMIS_Windsor 103`)
CIMIS_Windsor_103$Tmin = NULL
CIMIS_Windsor_103$Tmax = NULL
#Finalize CIMIS Hopland 85 (just consists of -999)
CIMIS_Hopland_85 = cbind.data.frame(seq(from = StartDate$date, to = EndDate$date, by = 'day'),
rep ("Hopland_85", ndays), rep(-999,ndays))
colnames(CIMIS_Hopland_85) = c("Date", "Station", "Precipitation")
CIMIS_Hopland_85$Date = as.character(CIMIS_Hopland_85$Date) #convert dates to characters
CIMIS_Hopland_85$Date = gsub("-", "", CIMIS_Hopland_85$Date) # remove dashes from dates
View(CIMIS_Hopland_85)
View(CIMIS_Sanel_Valley_106)
View(CIMIS_Santa_Rosa_83)
View(CIMIS_Windsor_103)
View(CIMIS_Hopland_85)
View(CIMIS_Sanel_Valley_106)
View(CIMIS_Santa_Rosa_83)
list_df = list(CIMIS_Hopland_85, CIMIS_Sanel_Valley_106, CIMIS_Santa_Rosa_83, CIMIS_Windsor_103)
View(list_df)
CIMIS_Processed = list_df %>% reduce(inner_join, by='Date')
View(CIMIS_Processed)
CIMIS_Names = c("Date", "Hopland", "Hopland_85_PRECIP6", "Sanel Valley", "Sanel_Valley_106_TMAX3", "Sanel_Valley_106_TMIN3", "Santa Rosa",
"Santa_Rosa_83_TMAX4", "Santa_Rosa_83_TMIN4", "Windsor", "Windsor_103_PRECIP12")
colnames(CIMIS_Processed) = CIMIS_Names
colnames(CIMIS_Processed)
CIMIS_Processed = select(CIMIS_Processed, -c("Hopland", "Sanel Valley", "Santa Rosa", "Windsor"))
col_order = c("Date", "Hopland_85_PRECIP6", "Windsor_103_PRECIP12", "Sanel_Valley_106_TMAX3", "Sanel_Valley_106_TMIN3", "Santa_Rosa_83_TMAX4", "Santa_Rosa_83_TMIN4")
CIMIS_Processed = CIMIS_Processed[,col_order]
CIMIS_Processed
View(CIMIS_Processed)
# Set up RSelenium -------------------------------------------------------
## load packages ----
library(RSelenium)
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
# Import CNRFC Temperature stations----
CNRFC_Stations <- read.csv(here("InputData/CNRFC_Stations.csv"))
##Set Default download folder ----
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
default_folder <- eCaps$chromeOptions$prefs$download.default_directory
#
## Find active versions of chrome on PC ----
binman::list_versions('chromedriver')
## Open a chrome browser session with RSelenium ----
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='111.0.5563.64', #set to the version on your PC that most closely matches the chrome browser version
port = free_port(),
extraCapabilities = eCaps
)
remDr <- rs_driver_object$client
remDr$open()
#Navigate to CNRFC website
for (i in 1:nrow(CNRFC_Stations)){
CNRFC <- paste0("https://www.cnrfc.noaa.gov/temperaturePlots_hc.php?id=", CNRFC_Stations$TempStation[i])
remDr$navigate(CNRFC)
#Select Chart Menu
ChartMenu <- remDr$findElement(using = "xpath", "//button[@aria-label = 'View chart menu']")
ChartMenu$clickElement()
#Download as CSV
CSVDownload <- remDr$findElement(using = "xpath", "//ul//li[contains(., 'CSV')]")
CSVDownload$clickElement()
}
#Load libraries----
library(here)
library(dplyr)
library(tidyverse)
library(lubridate)
#PRISM Precipitation Data Manipulation----
#Import PRISM_Precipitation.csv by skipping first 10 rows
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
names(PP)[6] = "ppt"
#Remove unnecessary columns
PP = select(PP, c("Name", "ppt"))
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP$Name) #vector of unique Prism station names
#Replace old Prism station names with new names
PP$Name <- PP_NewNames[match(PP$Name,PP_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PP$Name)) {
assign(i, PP %>% filter (Name == i), envir = .GlobalEnv)
}
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[5:6] = c("Tmin", "Tmax")
#Remove unnecessary columns
PT <- select(PT, c("Name", "Tmin", "Tmax"))
##Create separate dataframes for each Prism temperature station----
PT_NewNames <- c("Temp1", "Temp2", "Temp3", "Temp4", "Temp5", "Temp6",
"Temp7", "Temp8")
PT_OldNames <-unique(PT$Name)
#Replace Old Prism station names with new names
PT$Name <- PT_NewNames[match(PT$Name,PT_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PT$Name)) {
assign(i, PT %>% filter (Name == i), envir = .GlobalEnv)
}
View(PP_PRECIP8)
