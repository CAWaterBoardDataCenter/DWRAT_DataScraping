WeatherDataBody <-select(WeatherDataBody, -c(X8:X19))
#Add column headers to WeatherDataBody
Headers <- c("Station","Date","Time","Precip","type","Tmax","Tmin")
colnames(WeatherDataBody) = Headers
#Drop Time and type columns
WeatherDataBody <- select(WeatherDataBody, -c("Time", "type"))
DF_List[[i]] <- WeatherDataBody
}
#Finalize CIMIS Data For Exportation To CSV----
#Name the individual RAWS dataframes in DF_List
names(DF_List) <- lapply(seq_along(DF_List),
function(i) names(DF_List)[[i]] = paste0("CIMIS_", Stations$Station[i]))
#Extract dataframes from DF_List
lapply(names(DF_List), function(i)
assign(x = i, value = DF_List[[i]], .GlobalEnv))
#Finalize CIMIS Sanel Valley 106
CIMIS_Sanel_Valley_106 = `CIMIS_Sanel Valley 106`
rm(`CIMIS_Sanel Valley 106`)
CIMIS_Sanel_Valley_106$Precip = NULL
#Finalize CIMIS Santa Rosa 83
CIMIS_Santa_Rosa_83 = `CIMIS_Santa Rosa 83`
rm(`CIMIS_Santa Rosa 83`)
CIMIS_Santa_Rosa_83$Precip = NULL
#Finalize CIMIS Windsor 103
CIMIS_Windsor_103 = `CIMIS_Windsor 103`
rm(`CIMIS_Windsor 103`)
CIMIS_Windsor_103$Tmin = NULL
CIMIS_Windsor_103$Tmax = NULL
#Finalize CIMIS Hopland 85 (just consists of -999)
CIMIS_Hopland_85 = cbind.data.frame(seq(from = StartDate$date, to = EndDate$date, by = 'day'),
rep ("Hopland_85", ndays), rep(-999,ndays))
colnames(CIMIS_Hopland_85) = c("Date", "Station", "Precipitation")
CIMIS_Hopland_85$Date = as.character(CIMIS_Hopland_85$Date) #convert dates to characters
CIMIS_Hopland_85$Date = gsub("-", "", CIMIS_Hopland_85$Date) # remove dashes from dates
##Consolidate the CIMIS datasets into a single dataframe----
list_df = list(CIMIS_Hopland_85, CIMIS_Sanel_Valley_106, CIMIS_Santa_Rosa_83, CIMIS_Windsor_103)
CIMIS_Processed = list_df %>% reduce(inner_join, by='Date')
CIMIS_Names = c("Date", "Hopland", "Hopland_85_PRECIP6", "Sanel Valley", "Sanel_Valley_106_TMAX3", "Sanel_Valley_106_TMIN3", "Santa Rosa",
"Santa_Rosa_83_TMAX4", "Santa_Rosa_83_TMIN4", "Windsor", "Windsor_103_PRECIP12")
colnames(CIMIS_Processed) = CIMIS_Names
colnames(CIMIS_Processed)
CIMIS_Processed = select(CIMIS_Processed, -c("Hopland", "Sanel Valley", "Santa Rosa", "Windsor"))
col_order = c("Date", "Hopland_85_PRECIP6", "Windsor_103_PRECIP12", "Sanel_Valley_106_TMAX3", "Sanel_Valley_106_TMIN3", "Santa_Rosa_83_TMAX4", "Santa_Rosa_83_TMIN4")
CIMIS_Processed = CIMIS_Processed[,col_order]
CIMIS_Processed
#Replace all missing values with -999
CIMIS_Processed[CIMIS_Processed == ""] = -999
#InputData----
#Dates--adjust as needed; EndDate is always yesterday
Stations <- read.csv(here("InputData/CIMIS_Stations.csv"))
StartDate = data.frame("January", "11", "2023", as.Date("2023-01-11"))
EndDate = data.frame("March", "19", "2023", as.Date("2023-03-19"))
colnames(StartDate) = c("month", "day", "year", "date")
colnames(EndDate) = c("month", "day", "year", "date")
ndays = seq(from = StartDate$date, to = EndDate$date, by = 'day')%>% length()
ndays
# Set up RSelenium ----
# Open a chrome browser session with RSelenium
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='111.0.5563.64',
port = free_port(),
)
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
remDr <- rs_driver_object$client
remDr$open()
#Create a list to hold CIMIS dataframes
DF_List <- list()
#Navigate to CIMIS----
for (i in 1:nrow(Stations)){
#i=1
URL <- paste0("https://ipm.ucanr.edu/calludt.cgi/WXSTATIONDATA?MAP=&STN=", Stations$Alias[i])
URL <- toString(URL)
remDr$navigate(URL)
#Input Dates
StartMonth <- remDr$findElement(using = "name", value = "FROMMONTH")
StartMonth$sendKeysToElement(list(StartDate$month))
StartDay <- remDr$findElement(using = "name", value = "FROMDAY")
StartDay$sendKeysToElement(list(StartDate$day))
StartYear <- remDr$findElement(using = "name", value = "FROMYEAR")
StartYear$sendKeysToElement(list(StartDate$year))
EndMonth <- remDr$findElement(using = "name", value = "THRUMONTH")
EndMonth$sendKeysToElement(list(EndDate$month))
EndDay <-remDr$findElement(using = "name", value = "THRUDAY")
EndDay$sendKeysToElement(list(EndDate$day))
EndYear <-remDr$findElement(using = "name", value = "THRUYEAR")
EndYear$sendKeysToElement(list(EndDate$year))
#Use no backups
Backups <- remDr$findElement(using = "name", value = "NONE")
Backups$clickElement()
#Uncheck unnecessary checkboxes
Soil <- remDr$findElement(using = "name", value = "DT_SOIL")
Soil$clickElement()
Wind <- remDr$findElement(using = "name", value = "DT_WIND")
Wind$clickElement()
RH <- remDr$findElement(using = "name", value = "DT_RH")
RH$clickElement()
ET <- remDr$findElement(using = "name", value = "DT_ET")
ET$clickElement()
Solar <- remDr$findElement(using = "name", value = "DT_SOLAR")
Solar$clickElement()
#Metric Units
Metric <- remDr$findElement(using = "xpath", "//input[@value = 'M']")
Metric$clickElement()
#Comma delimited format
Comma <- remDr$findElement(using = "xpath", "//input[@value = 'T']")
Comma$clickElement()
#Retrieve Report
Report <-remDr$findElement(using = "xpath", "//input[@value = 'RETRIEVE DATA']")
Report$clickElement()
#Grab the Data
WeatherData <- remDr$findElement(using = "xpath", "//pre")
WeatherDataText <-WeatherData$getElementText() %>% unlist() %>% data.frame()
#Manipulate CIMIS Data After Download----
WeatherDataBody <- substring(WeatherDataText, 2551, nchar(WeatherDataText))
WeatherDataBody <-gsub("\\\n", " ", WeatherDataBody) #Remove \n from
WeatherDataBody <-gsub(" ", "", WeatherDataBody) #remove blank spaces
WeatherDataBody <- strsplit( WeatherDataBody, ",") %>% unlist %>% data.frame() #split by commas
#Force WeatherDataBody into a dataframe with 19 columns
WeatherDataBody <- split(WeatherDataBody,rep(1:(nrow(WeatherDataBody)/19),each=19)) %>% data.frame %>% t() %>% data.frame()
#Drop the last 12 columns
WeatherDataBody <-select(WeatherDataBody, -c(X8:X19))
#Add column headers to WeatherDataBody
Headers <- c("Station","Date","Time","Precip","type","Tmax","Tmin")
colnames(WeatherDataBody) = Headers
#Drop Time and type columns
WeatherDataBody <- select(WeatherDataBody, -c("Time", "type"))
DF_List[[i]] <- WeatherDataBody
}
#Finalize CIMIS Data For Exportation To CSV----
#Name the individual RAWS dataframes in DF_List
names(DF_List) <- lapply(seq_along(DF_List),
function(i) names(DF_List)[[i]] = paste0("CIMIS_", Stations$Station[i]))
#Extract dataframes from DF_List
lapply(names(DF_List), function(i)
assign(x = i, value = DF_List[[i]], .GlobalEnv))
#Finalize CIMIS Sanel Valley 106
CIMIS_Sanel_Valley_106 = `CIMIS_Sanel Valley 106`
rm(`CIMIS_Sanel Valley 106`)
CIMIS_Sanel_Valley_106$Precip = NULL
#Finalize CIMIS Santa Rosa 83
CIMIS_Santa_Rosa_83 = `CIMIS_Santa Rosa 83`
rm(`CIMIS_Santa Rosa 83`)
CIMIS_Santa_Rosa_83$Precip = NULL
#Finalize CIMIS Windsor 103
CIMIS_Windsor_103 = `CIMIS_Windsor 103`
rm(`CIMIS_Windsor 103`)
CIMIS_Windsor_103$Tmin = NULL
CIMIS_Windsor_103$Tmax = NULL
#Finalize CIMIS Hopland 85 (just consists of -999)
CIMIS_Hopland_85 = cbind.data.frame(seq(from = StartDate$date, to = EndDate$date, by = 'day'),
rep ("Hopland_85", ndays), rep(-999,ndays))
colnames(CIMIS_Hopland_85) = c("Date", "Station", "Precipitation")
CIMIS_Hopland_85$Date = as.character(CIMIS_Hopland_85$Date) #convert dates to characters
CIMIS_Hopland_85$Date = gsub("-", "", CIMIS_Hopland_85$Date) # remove dashes from dates
##Consolidate the CIMIS datasets into a single dataframe----
list_df = list(CIMIS_Hopland_85, CIMIS_Sanel_Valley_106, CIMIS_Santa_Rosa_83, CIMIS_Windsor_103)
CIMIS_Processed = list_df %>% reduce(inner_join, by='Date')
CIMIS_Names = c("Date", "Hopland", "Hopland_85_PRECIP6", "Sanel Valley", "Sanel_Valley_106_TMAX3", "Sanel_Valley_106_TMIN3", "Santa Rosa",
"Santa_Rosa_83_TMAX4", "Santa_Rosa_83_TMIN4", "Windsor", "Windsor_103_PRECIP12")
colnames(CIMIS_Processed) = CIMIS_Names
colnames(CIMIS_Processed)
CIMIS_Processed = select(CIMIS_Processed, -c("Hopland", "Sanel Valley", "Santa Rosa", "Windsor"))
col_order = c("Date", "Hopland_85_PRECIP6", "Windsor_103_PRECIP12", "Sanel_Valley_106_TMAX3", "Sanel_Valley_106_TMIN3", "Santa_Rosa_83_TMAX4", "Santa_Rosa_83_TMIN4")
CIMIS_Processed = CIMIS_Processed[,col_order]
CIMIS_Processed
#Replace all missing values with -999
CIMIS_Processed[CIMIS_Processed == ""] = -999
system("taskkill /im java.exe /f")
View(CIMIS_Processed)
write.csv(CIMIS_Processed, here("ProcessedData/CIMIS_Processed.csv"), row.names = FALSE)
#THIS SCRIPT IS EXPERIMENTAL AND DOES NOT FULLY WORK YET#
#LAST UPDATED BY: PAYMAN ALEMI ON 1/11/2023
## load packages
library(tidyverse)
library(netstat)
library(here)
library(dplyr)
library(readr)
library(lubridate)
#Import Data
Stations = read.csv(here("InputData/CNRFC_Stations.csv"))
CNRFC_precip = read.csv(here("WebData/cnrfc_qpf.csv"), header = FALSE)
CNRFC_precip$V28 = NULL
nchar(CNRFC_precip[2,4])
#Create data frames for start dates and end dates
StartDates = mdy_hm(CNRFC_precip[2,4:27]) %>% data.frame()
colnames(StartDates) = "Date"
StartDates = date(StartDates$Date) %>% data.frame()
EndDates = mdy_hm(CNRFC_precip[3,4:27]) %>% data.frame()
colnames(EndDates) = "Date"
EndDates = date(EndDates$Date) %>% data.frame()
#Filter CNRFC_precipitation to just the stations we care about
CNRFC_precip_data = inner_join(x = CNRFC_precip,
y = Stations,
by = c("V1" = "PrecipStation"))
#Drop unnecessary columns
CNRFC_precip_data$V2 = NULL
CNRFC_precip_data$V3 = NULL
CNRFC_precip_data$TempStation = NULL
#Convert precipitation from inches to mm
CNRFC_precip_data[,2:25]= sapply(CNRFC_precip_data[,2:25], as.numeric)
CNRFC_precip_data[,2:25] = CNRFC_precip_data[,2:25]*25.4
#Transpose CNRFC_precip_data
CNRFC_precip_data = t(CNRFC_precip_data) %>% data.frame()
#Replace column names
colnames(CNRFC_precip_data) = CNRFC_precip_data[1,]
#Delete 1st row of data
#Add dates in 1st column
#Sum precipitation by date for each station
#Write to CSV
write.csv(CNRFC_precip_data, here("ProcessedData/CNRFC_Precip_Data.csv"), row.names= FALSE)
# Import CNRFC Temperature stations----
CNRFC_Stations <- read.csv(here("InputData/CNRFC_Stations.csv"))
##Set Default download folder ----
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
default_folder <- eCaps$chromeOptions$prefs$download.default_directory
#
## Find active versions of chrome on PC ----
binman::list_versions('chromedriver')
## Open a chrome browser session with RSelenium ----
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='111.0.5563.64', #set to the version on your PC that most closely matches the chrome browser version
port = free_port(),
extraCapabilities = eCaps
)
remDr <- rs_driver_object$client
remDr$open()
#Navigate to CNRFC website
for (i in 1:nrow(CNRFC_Stations)){
CNRFC <- paste0("https://www.cnrfc.noaa.gov/temperaturePlots_hc.php?id=", CNRFC_Stations$TempStation[i])
remDr$navigate(CNRFC)
#Select Chart Menu
ChartMenu <- remDr$findElement(using = "xpath", "//button[@aria-label = 'View chart menu']")
ChartMenu$clickElement()
#Download as CSV
CSVDownload <- remDr$findElement(using = "xpath", "//ul//li[contains(., 'CSV')]")
CSVDownload$clickElement()
}
system("taskkill /im java.exe /f")
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP$Name) #vector of unique Prism station names
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
names(PP)[6] = "ppt"
#Remove unnecessary columns
PP = select(PP, c("Name", "ppt"))
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
names(PP)[6] = "ppt"
#Remove unnecessary columns
PP = select(PP, c("Name", "ppt"))
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP$Name) #vector of unique Prism station names
#Replace old Prism station names with new names
PP$Name <- PP_NewNames[match(PP$Name,PP_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PP$Name)) {
assign(i, PP %>% filter (Name == i), envir = .GlobalEnv)
}
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[5:6] = c("Tmin", "Tmax")
#Remove unnecessary columns
PT <- select(PT, c("Name", "Tmin", "Tmax"))
##Create separate dataframes for each Prism temperature station----
PT_NewNames <- c("Temp1", "Temp2", "Temp3", "Temp4", "Temp5", "Temp6",
"Temp7", "Temp8")
PT_OldNames <-unique(PT$Name)
#Replace Old Prism station names with new names
PT$Name <- PT_NewNames[match(PT$Name,PT_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PT$Name)) {
assign(i, PT %>% filter (Name == i), envir = .GlobalEnv)
}
View(PP_PRECIP1)
?require
View(CNRFC_precip)
View(CNRFC_precip_data)
View(CNRFC_precip_data)
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
names(PP)[6] = "ppt"
View(PP)
#Remove unnecessary columns
PP = select(PP, c("Name", "ppt"))
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP$Name) #vector of unique Prism station names
PP_OldNames
PP$Name <- PP_NewNames[match(PP$Name,PP_OldNames, nomatch = 0)
]
View(PP_PRECIP1)
View(PP_PRECIP10)
for (i in unique(PP$Name)) {
assign(i, PP %>% filter (Name == i), envir = .GlobalEnv)
}
View(PP_PRECIP1)
system("taskkill /im java.exe /f")
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
names(PP)[6] = "ppt"
View(PP)
#Remove unnecessary columns
PP = select(PP, c("Name", "ppt"))
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP$Name) #vector of unique Prism station names
#Replace old Prism station names with new names
PP$Name <- PP_NewNames[match(PP$Name,PP_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PP$Name)) {
assign(i, PP %>% filter (Name == i), envir = .GlobalEnv)
}
View(PP_PRECIP1)
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[5:6] = c("Tmin", "Tmax")
View(PT)
#Remove unnecessary columns
PT <- select(PT, c("Name", "Tmin", "Tmax"))
View(PT)
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[5:6] = c("Tmin", "Tmax")
View(PT)
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[6:7] = c("Tmin", "Tmax")
#Remove unnecessary columns
PT <- select(PT, c("Name", "Tmin", "Tmax"))
##Create separate dataframes for each Prism temperature station----
PT_NewNames <- c("Temp1", "Temp2", "Temp3", "Temp4", "Temp5", "Temp6",
"Temp7", "Temp8")
PT_OldNames <-unique(PT$Name)
#Replace Old Prism station names with new names
PT$Name <- PT_NewNames[match(PT$Name,PT_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PT$Name)) {
assign(i, PT %>% filter (Name == i), envir = .GlobalEnv)
}
View(Temp1)
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[6:7] = c("Tmin", "Tmax")
#PRISM Temperature Data Manipulation----
#Import Prism_Temp.csv by skipping first 10 rows
PT <- read.csv(here("WebData/PRISM_Temperature.csv"), skip = 10, header = T)
names(PT)[6:7] = c("Tmin", "Tmax")
#PRISM Precipitation Data Manipulation----
#Import PRISM_Precipitation.csv by skipping first 10 rows
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
View(PP)
names(PP)[6] = "ppt"
#Remove unnecessary columns
PP = select(PP, c("Name", "ppt"))
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP$Name) #vector of unique Prism station names
#Replace old Prism station names with new names
PP$Name <- PP_NewNames[match(PP$Name,PP_OldNames, nomatch = 0)]
#Extract each station as a separate dataframe
for (i in unique(PP$Name)) {
assign(i, PP %>% filter (Name == i), envir = .GlobalEnv)
}
#Import DAT stuff
DAT_File <- read.delim(here("InputData/data_update_to_11.30.2022.txt"), sep = "\t")
DAT_Fields <- read.csv(here("InputData/DAT_Fields.csv"))
View(DAT_Fields)
View(DAT_File)
View(DAT_File)
#Set DAT_File column names
colnames(DAT_File) <- colnames(DAT_Fields)
#Whittle the DAT_File to a subset corresponding to our timeframe of interest, "DAT_Shell"----
#Add a Timestep column
DAT_File$TimeStep <- make_date(year = DAT_File$Year,
month = DAT_File$month,
day = DAT_File$day)
#Filter to the timeframe of interest
#Set the start date by grabbing the first day of the current month
#Start_Date <- lubridate::floor_date(Sys.Date(), unit = "month")
Start_Date <- as.Date("2023-01-11")
View(StartDate)
#The end date is the current date + 5 days in the future; we grab 6 days of forecast data from CNRFC
End_Date <- Sys.Date() + 5
View(EndDates)
View(EndDate)
View(EndDate)
#The end date is the current date + 5 days in the future; we grab 6 days of forecast data from CNRFC
End_Date <- Sys.Date() + 5
View(EndDate)
End_Date
View(EndDate)
DAT_Shell <- subset(DAT_File, TimeStep >= '2023-01-11' & TimeStep <= "2023-03-25") #Adjust as needed
DAT_Shell
View(DAT_Shell)
#Set TimeStep as the 7th column in DAT_Shell
DAT_Shell <- DAT_Shell %>% relocate(TimeStep, .after = s)
#Set all the DAT_Shell temperature and precipitation fields to blank
for (i in 1:length(DAT_Shell)){
if (colnames(DAT_Shell[i]) %>% str_detect("PREC")){
DAT_Shell[i] = ""
} else if (colnames(DAT_Shell[i]) %>% str_detect("TM")){
DAT_Shell[i] = ""
}else {
DAT_Shell[i] = DAT_Shell[i]
}
}
CIMIS_Processed
View(CIMIS_Processed)
# Import CNRFC Temperature stations----
CNRFC_Stations <- read.csv(here("InputData/CNRFC_Stations.csv"))
##Set Default download folder ----
eCaps <- list(
chromeOptions =
list(prefs = list(
"profile.default_content_settings.popups" = 0L,
"download.prompt_for_download" = FALSE,
"download.default_directory" = gsub(pattern = '/', replacement = '\\\\', x = here("WebData")) # download.dir
)
)
)
default_folder <- eCaps$chromeOptions$prefs$download.default_directory
#
## Find active versions of chrome on PC ----
binman::list_versions('chromedriver')
## Open a chrome browser session with RSelenium ----
rs_driver_object <-rsDriver(
browser = 'chrome',
chromever ='111.0.5563.64', #set to the version on your PC that most closely matches the chrome browser version
port = free_port(),
extraCapabilities = eCaps
)
remDr <- rs_driver_object$client
remDr$open()
#Navigate to CNRFC website
for (i in 1:nrow(CNRFC_Stations)){
CNRFC <- paste0("https://www.cnrfc.noaa.gov/temperaturePlots_hc.php?id=", CNRFC_Stations$TempStation[i])
remDr$navigate(CNRFC)
#Select Chart Menu
ChartMenu <- remDr$findElement(using = "xpath", "//button[@aria-label = 'View chart menu']")
ChartMenu$clickElement()
#Download as CSV
CSVDownload <- remDr$findElement(using = "xpath", "//ul//li[contains(., 'CSV')]")
CSVDownload$clickElement()
}
#Work in progess ----
#Combining CNRFC data with CIMIS data
CNRFC_precip_data <- read.csv(here("InputData/CNRFC_precip_data.csv"))
library(here)
library(dplyr)
library(tidyr)
library(tidyverse)
library(lubridate)
#PRISM Precipitation Data Manipulation----
#Import PRISM_Precipitation.csv by skipping first 10 rows
ndays = 68
PP <- read.csv(here("WebData/PRISM_Precipitation.csv"), skip = 10, header = T)
names(PP)[1] = "Station"
names(PP)[6] = "ppt"
#Remove unnecessary columns
PP = select(PP, c("Station", "Date", "ppt"))
#Pivot PP so that each station becomes a separate column
PP <- pivot_wider(PP, id_cols = Date, names_from = Station, values_from = ppt)
##create separate dataframes for each Prism precipitation station----
#Create a vector consisting of each station's new name
PP_NewNames <- c("Date", "PP_PRECIP1", "PP_PRECIP2", "PP_PRECIP3", "PP_PRECIP4", "PP_PRECIP5",
"PP_PRECIP6", "PP_PRECIP7", "PP_PRECIP8", "PP_PRECIP9", "PP_PRECIP10",
"PP_PRECIP11", "PP_PRECIP12", "PP_PRECIP13", "PP_PRECIP14", "PP_PRECIP15")
PP_OldNames <- unique(PP) #vector of unique Prism station names
colnames(PP) = PP_NewNames
#Export PP to CSV
write.csv(here("WebData/PRISM_Precipitation.csv", row.names = FALSE))
View(PP)
View(PP_OldNames)
#Export PP to CSV
write.csv(PP, here("WebData/PRISM_Precipitation.csv", row.names = FALSE))
