{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Water Usage Report Submittal Percentage Script**\n",
    "\n",
    "Consists of 3 main blocks:\n",
    "\n",
    "1. Connecting to the ReportManager, 1542 server, Report DB databse, which hosts all the ewrims flat files\n",
    "1. Linking to the Main_sheet of the Watershed Demand Dataset Paths spreadsheet on SharePoint\n",
    "1. Generic Function Definition for generating_reporting_summary--this function calculates the percentage of eligible rights\n",
    "    that reported for a given watershed; its input arguments are:\n",
    "    * pods_df--this is the POD list for the watershed after the completion of the GIS manual review \n",
    "    * watershed_code--this is the 2-character code for the watershed, \"NV\" for the Navarro Watershed for example\n",
    "    * conn--this is the connection mechanism to the server and database with the flat files\n",
    "    * output_folder--this is the folder where we will export all the summary tables generated by this script\n",
    "1. Function call that iterates through each each watershed listed in the Watershed Demand Dataset Paths spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Define all paths and load the Watershed Demand Dataset Paths (\"Driver\") spreadsheet\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "username = os.getenv(\"USERNAME\")\n",
    "\n",
    "# Set working directory to the DWRAT_DataScraping\\Demand folder\n",
    "project_root = fr\"C:\\Users\\{username}\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\"\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Define the path to the Watershed Demand Dataset Paths (\"Driver\") spreadsheet\n",
    "sharepoint_path = fr\"C:\\Users\\{username}\\Water Boards\\Supply and Demand Assessment - Documents\\Program Watersheds\\4. Demand Data Tracking\\Watershed_Demand_Dataset_Paths.xlsx\"\n",
    "\n",
    "# Load the driver spreadsheet\n",
    "driver_df = pd.read_excel(sharepoint_path, skiprows=1, sheet_name = \"Main_Sheet\")\n",
    "driver_df.columns\n",
    "\n",
    "# Ensure that the \n",
    "driver_df['POD_APPLICATION_NUMBER_SPREADSHEET_PATH'] = (\n",
    "    driver_df['POD_APPLICATION_NUMBER_SPREADSHEET_PATH']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Connect to ReportManager, 1542 server and the ReportDB database\n",
    "import pyodbc\n",
    "\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=reportmanager,1542;'\n",
    "    'DATABASE=ReportDB;'\n",
    "    'Trusted_Connection=yes;'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a single watershed to debug\n",
    "row = driver_df.iloc[11]  # Or use .loc if you want by watershed_code\n",
    "\n",
    "watershed_code = row['ID']\n",
    "sheet_name = row['POD_APPLICATION_NUMBER_WORKSHEET_NAME']\n",
    "partial_path = row['POD_APPLICATION_NUMBER_SPREADSHEET_PATH']\n",
    "\n",
    "username = os.getenv(\"USERNAME\")\n",
    "base_path = fr\"C:\\Users\\{username}\\Water Boards\\Supply and Demand Assessment - Documents\"\n",
    "full_path = os.path.join(base_path, partial_path)\n",
    "\n",
    "print(f\"üß™ Watershed: {watershed_code}\")\n",
    "print(f\"üß™ Path: {full_path}\")\n",
    "\n",
    "pods_df = pd.read_excel(full_path, sheet_name=sheet_name)\n",
    "pods_df.head()\n",
    "\n",
    "pods_df['APPLICATION_NUMBER'] = pods_df['APPLICATION_NUMBER'].astype(str).str.strip()\n",
    "pods_df = pods_df.drop_duplicates(subset='APPLICATION_NUMBER') # remove duplicate water rights\n",
    "pods_df = pods_df.rename(columns={'APPLICATION_NUMBER': 'app_number'}) # Rename the APPLICATION_NUMBER column\n",
    "pods_df = pods_df[['app_number']] # Remove all columns except for app_number\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT Application_Number AS app_number, Effective_Date\n",
    "FROM ReportDB.FLAT_FILE.ewrims_flat_file\n",
    "\"\"\"\n",
    "ewrims_df = pd.read_sql(query, conn)\n",
    "ewrims_df['app_number'] = ewrims_df['app_number'].astype(str).str.strip()\n",
    "\n",
    "joined_df = pd.merge(pods_df, ewrims_df, on='app_number', how='inner')\n",
    "joined_df['Effective_Date'] = pd.to_datetime(joined_df['Effective_Date'], errors='coerce')\n",
    "joined_df.head()\n",
    "\n",
    "for year in range(2017, 2025):\n",
    "    cutoff = pd.to_datetime(f'{year - 1}-12-31')\n",
    "    joined_df[f'Eligible_{year}'] = joined_df['Effective_Date'].apply(\n",
    "        lambda x: 'Y' if pd.notnull(x) and x <= cutoff else 'N'\n",
    "    )\n",
    "\n",
    "app_numbers = joined_df['app_number'].unique().tolist()\n",
    "query_reports = f\"\"\"\n",
    "SELECT APPL_ID AS app_number, YEAR\n",
    "FROM ReportDB.FLAT_FILE.ewrims_water_use_report\n",
    "WHERE YEAR BETWEEN 2017 AND 2024\n",
    "AND APPL_ID IN ({','.join(\"'\" + x + \"'\" for x in app_numbers)})\n",
    "\"\"\"\n",
    "report_df = pd.read_sql(query_reports, conn)\n",
    "report_df['app_number'] = report_df['app_number'].astype(str).str.strip()\n",
    "report_df['YEAR'] = report_df['YEAR'].astype(int) # Convert the Year column to an integer type\n",
    "\n",
    "year = 2024\n",
    "eligible_set = set(joined_df[joined_df[f'Eligible_{year}'] == 'Y']['app_number'])\n",
    "reported_set = set(report_df[report_df['YEAR'] == year]['app_number'])\n",
    "\n",
    "print(f\"‚úÖ {year} ‚Äî Eligible: {len(eligible_set)}, Reported: {len(reported_set)}, Matched: {len(eligible_set & reported_set)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define the generate_reporting_summary function\n",
    "def generate_reporting_summary(pods_df, watershed_code, conn, output_folder):\n",
    "    # 1. Clean and standardize APPLICATION_NUMBER\n",
    "    pods_df['APPLICATION_NUMBER'] = pods_df['APPLICATION_NUMBER'].astype(str).str.strip()\n",
    "    pods_df = pods_df.drop_duplicates(subset='APPLICATION_NUMBER') # Remove duplicate water rights\n",
    "    pods_df = pods_df.rename(columns={'APPLICATION_NUMBER': 'app_number'}) \n",
    "    pods_df = pods_df[['app_number']] # Remove all columns except for app_number\n",
    "    \n",
    "    # 2. Load Effective_Date from ewrims_flat_file\n",
    "    query_flat_file = \"\"\"\n",
    "    SELECT DISTINCT Application_Number AS app_number, Effective_Date\n",
    "    FROM ReportDB.FLAT_FILE.ewrims_flat_file\n",
    "    \"\"\"\n",
    "    ewrims_df = pd.read_sql(query_flat_file, conn)\n",
    "    ewrims_df['app_number'] = ewrims_df['app_number'].astype(str).str.strip()\n",
    "\n",
    "    # 3. Inner join to get Effective_Date\n",
    "    joined_df = pd.merge(pods_df, ewrims_df, on='app_number', how='inner')\n",
    "    joined_df['Effective_Date'] = pd.to_datetime(joined_df['Effective_Date'], errors='coerce')\n",
    "    joined_df['app_number'] = joined_df['app_number'].astype(str).str.strip()\n",
    "   \n",
    "\n",
    "    # 4. Create eligibility flags\n",
    "    for year in range(2017, 2025):\n",
    "        cutoff = pd.to_datetime(f'{year-1}-12-31')\n",
    "        joined_df[f'Eligible_{year}'] = joined_df['Effective_Date'].apply(\n",
    "            lambda x: 'Y' if pd.notnull(x) and x <= cutoff else 'N'\n",
    "        )\n",
    "\n",
    "    # 5. Create temp table of app_numbers\n",
    "    app_numbers = joined_df['app_number'].dropna().unique().tolist()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"IF OBJECT_ID('tempdb..#AppNumbers') IS NOT NULL DROP TABLE #AppNumbers\")\n",
    "    cursor.execute(\"CREATE TABLE #AppNumbers (app_number NVARCHAR(50))\")\n",
    "    for app in app_numbers:\n",
    "        cursor.execute(\"INSERT INTO #AppNumbers (app_number) VALUES (?)\", app)\n",
    "    conn.commit()\n",
    "\n",
    "    # 6. Query water_use_report for matching applications\n",
    "    query_reports = \"\"\"\n",
    "    SELECT APPL_ID AS app_number, YEAR\n",
    "    FROM ReportDB.FLAT_FILE.ewrims_water_use_report\n",
    "    WHERE YEAR BETWEEN 2017 AND 2024\n",
    "    AND APPL_ID IN (SELECT app_number FROM #AppNumbers)\n",
    "    \"\"\"\n",
    "    report_df = pd.read_sql(query_reports, conn)\n",
    "    report_df['app_number'] = report_df['app_number'].astype(str).str.strip()\n",
    "    report_df['YEAR'] = report_df['YEAR'].astype(int) # Convert the Year column to an integer type\n",
    "\n",
    "    # 7. Calculate reporting summary\n",
    "    summary = []\n",
    "    for year in range(2017, 2025):\n",
    "        eligible_col = f'Eligible_{year}'\n",
    "        eligible_set = set(joined_df[joined_df[eligible_col] == 'Y']['app_number'])\n",
    "        reported_set = set(report_df[report_df['YEAR'] == year]['app_number'])\n",
    "        matched = eligible_set & reported_set\n",
    "        summary.append({\n",
    "            'Year': year,\n",
    "            'Eligible_Count': len(eligible_set),\n",
    "            'Reported_Count': len(matched),\n",
    "            'Reporting_Percentage': round(len(matched) / len(eligible_set) * 100, 2) if eligible_set else 0.0\n",
    "        })\n",
    "\n",
    "    # 8. Export summary\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    output_path = os.path.join(output_folder, f'reporting_summary_df_{watershed_code}.csv')\n",
    "    summary_df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úì Exported {watershed_code} summary to {output_path}\")\n",
    "    \n",
    "    print(f\"Sample app_number from joined_df: {joined_df['app_number'].iloc[0]} ({type(joined_df['app_number'].iloc[0])})\")\n",
    "    print(f\"Sample app_number from report_df: {report_df['app_number'].iloc[0]} ({type(report_df['app_number'].iloc[0])})\")\n",
    "\n",
    "    # Return the individual summary for collection\n",
    "    return summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing RR...\n",
      "‚ùå Failed for RR: Worksheet named 'RR_pod_points_Merge_filtered_PA' not found\n",
      "üöÄ Processing NV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported NV summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_NV.csv\n",
      "Sample app_number from joined_df: A016624 (<class 'str'>)\n",
      "Sample app_number from report_df: A009618 (<class 'str'>)\n",
      "üöÄ Processing NR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported NR summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_NR.csv\n",
      "Sample app_number from joined_df: A000631 (<class 'str'>)\n",
      "Sample app_number from report_df: A000631 (<class 'str'>)\n",
      "üöÄ Processing BC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported BC summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_BC.csv\n",
      "Sample app_number from joined_df: A000476 (<class 'str'>)\n",
      "Sample app_number from report_df: A000476 (<class 'str'>)\n",
      "üöÄ Processing TEST...\n",
      "‚ùå Failed for TEST: [Errno 2] No such file or directory: 'C:\\\\Users\\\\PAlemi\\\\Water Boards\\\\Supply and Demand Assessment - Documents\\\\Program Watersheds\\\\1. Watershed Folders\\\\Test Creek\\\\Test_POD_StreamStats_Review.xlsx'\n",
      "üöÄ Processing GL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported GL summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_GL.csv\n",
      "Sample app_number from joined_df: A011416 (<class 'str'>)\n",
      "Sample app_number from report_df: A011416 (<class 'str'>)\n",
      "üöÄ Processing SC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported SC summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_SC.csv\n",
      "Sample app_number from joined_df: A011566 (<class 'str'>)\n",
      "Sample app_number from report_df: A012034 (<class 'str'>)\n",
      "üöÄ Processing TD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported TD summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_TD.csv\n",
      "Sample app_number from joined_df: A004881 (<class 'str'>)\n",
      "Sample app_number from report_df: A004881 (<class 'str'>)\n",
      "üöÄ Processing TR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported TR summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_TR.csv\n",
      "Sample app_number from joined_df: A001029 (<class 'str'>)\n",
      "Sample app_number from report_df: A001205 (<class 'str'>)\n",
      "üöÄ Processing SFT...\n",
      "‚ùå Failed for SFT: [Errno 2] No such file or directory: 'C:\\\\Users\\\\PAlemi\\\\Water Boards\\\\Supply and Demand Assessment - Documents\\\\nan'\n",
      "üöÄ Processing MT...\n",
      "‚ùå Failed for MT: [Errno 2] No such file or directory: 'C:\\\\Users\\\\PAlemi\\\\Water Boards\\\\Supply and Demand Assessment - Documents\\\\nan'\n",
      "üöÄ Processing PC ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ewrims_df = pd.read_sql(query_flat_file, conn)\n",
      "C:\\Users\\palemi\\AppData\\Local\\Temp\\1\\ipykernel_20160\\187834552.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  report_df = pd.read_sql(query_reports, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported PC  summary to C:\\Users\\PAlemi\\Documents\\GitHub\\DWRAT_DataScraping\\Demand\\OutputData\\reporting_summary_df_PC .csv\n",
      "Sample app_number from joined_df: A000533 (<class 'str'>)\n",
      "Sample app_number from report_df: A000533 (<class 'str'>)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Iterate over driver_df and call the function\n",
    "all_summaries = []\n",
    "results = []\n",
    "\n",
    "for _, row in driver_df.iterrows():\n",
    "\n",
    "    watershed_code = row['ID']\n",
    "    sheet_name = row['POD_APPLICATION_NUMBER_WORKSHEET_NAME']\n",
    "\n",
    "    \n",
    "    output_folder = os.path.join(os.getcwd(), \"OutputData\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    username = os.getenv(\"USERNAME\")\n",
    "    partial_path = row['POD_APPLICATION_NUMBER_SPREADSHEET_PATH']\n",
    "\n",
    "      # ‚ö†Ô∏è Validate partial_path before attempting to use it\n",
    "    if pd.isna(partial_path) or not isinstance(partial_path, str):\n",
    "        print(f\"‚ö†Ô∏è Skipping {watershed_code} ‚Äî invalid or missing POD path: {repr(partial_path)}\")\n",
    "        results.append({'Watershed': watershed_code, 'Status': 'Skipped (Invalid path)'})\n",
    "        continue\n",
    "\n",
    "    base_path = fr\"C:\\Users\\{username}\\Water Boards\\Supply and Demand Assessment - Documents\"\n",
    "    full_path = os.path.join(base_path, partial_path)\n",
    "\n",
    "    print(f\"üöÄ Processing {watershed_code}...\")\n",
    "\n",
    "    try:\n",
    "        pods_df = pd.read_excel(io=full_path, sheet_name=sheet_name)\n",
    "        summary_df = generate_reporting_summary(pods_df, watershed_code, conn, output_folder)\n",
    "\n",
    "        # Add watershed code as a column in each summary\n",
    "        summary_df['Watershed'] = watershed_code\n",
    "        all_summaries.append(summary_df)\n",
    "\n",
    "        results.append({'Watershed': watershed_code, 'Status': 'Success'})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for {watershed_code}: {e}\")\n",
    "        results.append({'Watershed': watershed_code, 'Status': 'Failed', 'Error': str(e)})\n",
    "\n",
    "# Combine all per-watershed summaries into a master file\n",
    "master_df = pd.concat(all_summaries, ignore_index=True)\n",
    "master_df.to_csv(\"OutputData/master_reporting_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Russian River POD List and whittle it down\n",
    "import pandas as pd\n",
    "rr_pod_file_path = r'C:\\Users\\palemi\\Water Boards\\Supply and Demand Assessment - Documents\\Program Watersheds\\1. Watershed Folders\\Test Watersheds\\Test_Russian\\Data\\GIS_Preprocessing\\TR_GIS_Preprocessing_2025-03-17.xlsx'\n",
    "rr_pods = pd.read_excel(rr_pod_file_path, sheet_name = 'Final_POD_List')\n",
    "\n",
    "# Remove all columns except for APPLICATION_NUMBER\n",
    "\n",
    "rr_pods = rr_pods[['APPLICATION_NUMBER']]\n",
    "# Rename the APPLICATION_NUMBER field in rr_pods to app_num\n",
    "rr_pods.rename(columns={'APPLICATION_NUMBER': 'app_number'}, inplace=True)\n",
    "\n",
    "# Grab unique application numbers\n",
    "rr_pods = rr_pods.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inner Join rr_pods to the ewrims_flat_file SQL server table on the APPLICATION_NUMBER column\n",
    "query1 = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    Application_Number AS app_number,\n",
    "    Effective_Date\n",
    "FROM ReportDB.FLAT_FILE.ewrims_flat_file\n",
    "\"\"\"\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT DISTINCT *, Application_Number AS app_number\n",
    "FROM ReportDB.FLAT_FILE.ewrims_flat_file\n",
    "\"\"\"\n",
    "\n",
    "# Read the SQL query results into DataFrames\n",
    "ewrims_flat_file = pd.read_sql(query1, conn)\n",
    "ewrims_flat_file_all = pd.read_sql(query2, conn)\n",
    "\n",
    "# Preview the result\n",
    "print(ewrims_flat_file.head())\n",
    "print(ewrims_flat_file_all.head())\n",
    "\n",
    "# Perform inner join on APPLICATION_NUMBER\n",
    "rr_pods_flat_file= pd.merge(rr_pods, ewrims_flat_file, on='app_number', how='inner')\n",
    "rr_pods_flat_file_all = pd.merge(rr_pods,ewrims_flat_file_all, on = 'app_number', how = 'inner')\n",
    "\n",
    "print(f\"Number of matched water rights: {len(rr_pods_flat_file)}\")\n",
    "rr_pods_flat_file.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Effective_Date is a datetime object\n",
    "rr_pods_flat_file['Effective_Date'] = pd.to_datetime(rr_pods_flat_file['Effective_Date'], errors='coerce')\n",
    "\n",
    "# Define years for eligibility tagging\n",
    "years = list(range(2017, 2025))\n",
    "\n",
    "# Apply eligibility logic based on Effective_Date\n",
    "for year in years:\n",
    "    cutoff = pd.to_datetime(f'{year - 1}-12-31')\n",
    "    rr_pods_flat_file[f'Eligible_{year}'] = rr_pods_flat_file['Effective_Date'].apply(\n",
    "        lambda x: 'Y' if pd.notnull(x) and x <= cutoff else 'N'\n",
    "    )\n",
    "\n",
    "# Preview relevant columns\n",
    "columns_to_show = ['app_number', 'Effective_Date'] + [f'Eligible_{y}' for y in years]\n",
    "rr_pods_flat_file[columns_to_show].head(10)\n",
    "\n",
    "# Count Eligible Rights By Year--create a summary table\n",
    "\n",
    "# Initialize an empty list to collect summary data\n",
    "# Initialize an empty list to collect summary data\n",
    "eligibility_counts = []\n",
    "\n",
    "# Loop through each Eligible column and count the Ys\n",
    "for year in range(2017, 2025):\n",
    "    col_name = f'Eligible_{year}'\n",
    "    count = rr_pods_flat_file[col_name].value_counts().get('Y', 0)\n",
    "    eligibility_counts.append({'Year': year, 'Eligible_Count': count})  # <-- needs to be indented!\n",
    "\n",
    "\n",
    "eligibility_summary_df = pd.DataFrame(eligibility_counts)\n",
    "eligibility_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"SELECT TOP 5 * FROM ReportDB.FLAT_FILE.ewrims_water_use_report\"\n",
    "test_df = pd.read_sql(test_query, conn)\n",
    "\n",
    "# Print all column names\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ewrims_flat_file_water_report (but just the rights in the Russian River)\n",
    "\n",
    "# Get unique app_numbers in the Russian River as a list of strings\n",
    "app_numbers = rr_pods_flat_file['app_number'].dropna().unique().tolist()\n",
    "\n",
    "# Create a DataFrame of unique app_numbers\n",
    "app_numbers_df = pd.DataFrame({'app_number': app_numbers})\n",
    "\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Drop temp table if it exists (to be safe)\n",
    "cursor.execute(\"IF OBJECT_ID('tempdb..#AppNumbers') IS NOT NULL DROP TABLE #AppNumbers\")\n",
    "cursor.execute(\"CREATE TABLE #AppNumbers (app_number NVARCHAR(50))\")\n",
    "\n",
    "# Insert values into the temp table\n",
    "for app in app_numbers:\n",
    "    cursor.execute(\"INSERT INTO #AppNumbers (app_number) VALUES (?)\", app)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT Distinct\n",
    " R.APPL_ID As Application_Number, \n",
    " R.Year\n",
    "FROM ReportDB.FLAT_FILE.ewrims_water_use_report R\n",
    "INNER JOIN #AppNumbers A ON R.APPL_ID = A.app_number\n",
    "WHERE YEAR>= 2017\n",
    "\"\"\"\n",
    "\n",
    "report_df = pd.read_sql(query, conn)\n",
    "print(f\"Retrieved {len(report_df)} rows from filtered report table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING REPORTING PERCENTAGES\n",
    "\n",
    "# Initialize an empty list to contain the results\n",
    "reporting_summary = []\n",
    "\n",
    "# year = 2024 # Use this for a single iteration of the loop\n",
    "for year in range(2017,2025):\n",
    "    eligible_col = f'Eligible_{year}'\n",
    "\n",
    "    # Step 1: Get all eligible rights for this year\n",
    "    eligible_df = rr_pods_flat_file[rr_pods_flat_file[eligible_col] == 'Y']\n",
    "    eligible_apps = set(eligible_df['app_number'])\n",
    "\n",
    "    # Step 2: Get all reported rights for this year from the report table\n",
    "    report_df['Year'] = report_df['Year'].astype(int) # Convert the Year column to an integer type\n",
    "    reported_df_year = report_df[report_df['Year'] == year]\n",
    "    reported_apps = set(reported_df_year['Application_Number'])\n",
    "\n",
    "    #Step 3: Intersect the 2 datasets to find the eligible rights that submitted reports\n",
    "    matched_reports = eligible_apps & reported_apps\n",
    "\n",
    "    # Step 4: Count the reports and calculate the percentages\n",
    "    eligible_count = len(eligible_apps)\n",
    "    reported_count = len(matched_reports)\n",
    "    percentage = round(reported_count/eligible_count * 100, 2) if eligible_count > 0 else 0\n",
    "\n",
    "    # Step 5: Store the results in the reporting_summary list\n",
    "    reporting_summary.append({\n",
    "        'Year': year,\n",
    "        'Eligible_Count': eligible_count,\n",
    "        'Reported_Count': reported_count,\n",
    "        'Reporting_Percentage': percentage\n",
    "    })\n",
    "\n",
    "# Convert the reporting_summary list into a DataFrame\n",
    "reporting_summary_df = pd.DataFrame(reporting_summary)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"OutputData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export report_summary_df as a csv\n",
    "reporting_summary_df.to_csv(f\"{output_dir}/reporting_summary_df_RR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py4wrds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
